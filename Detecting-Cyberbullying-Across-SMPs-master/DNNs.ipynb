{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 20:27:13.235414: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 20:27:22.986253: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-09 20:27:22.986303: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-09 20:27:50.844055: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-09 20:27:50.844435: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-09 20:27:50.844472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/suhanik/.local/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from models import get_model\n",
    "import argparse\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import preprocessor as p\n",
    "from collections import Counter\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "#from tensorflow.contrib import learn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from scipy import stats\n",
    "import tflearn\n",
    "import json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = [] \n",
    "    for i in range(len(data)):\n",
    "        print(data[i]['text'])\n",
    "        if(HASH_REMOVE):\n",
    "            #x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
    "            x_text.append(p.tokenize((data[i]['text'])))\n",
    "        else:\n",
    "            x_text.append(data[i]['text'])\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global NUM_CLASSES, HASH_REMOVE\n",
    "    if(dataset==\"twitter\"):\n",
    "        NUM_CLASSES = 3\n",
    "        HASH_REMOVE = True\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "    elif(dataset==\"formspring\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "    elif(dataset==\"wiki\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_weights(filename, sep):\n",
    "    embed_dict = {}\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(sep)\n",
    "        embed_dict[row[0]] = row[1:]\n",
    "    print('Loaded from file: ' + str(filename))\n",
    "    file.close()\n",
    "    return embed_dict\n",
    "\n",
    "def map_embedding_weights(embed, vocab, embed_size):\n",
    "    vocab_size = len(vocab)\n",
    "    embeddingWeights = np.zeros((vocab_size , embed_size))\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingWeights[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    return embeddingWeights\n",
    "\n",
    "def get_embeddings_dict(vector_type, emb_dim):\n",
    "    if vector_type == 'sswe':\n",
    "        emb_dim==50\n",
    "        sep = '\\t'\n",
    "        vector_file = 'word_vectors/sswe-u.txt'\n",
    "    elif vector_type ==\"glove\":\n",
    "        sep = ' '\n",
    "        if data == \"wiki\":\n",
    "            vector_file = 'word_vectors/glove.6B.' + str(emb_dim) + 'd.txt'\n",
    "        else:\n",
    "            vector_file = 'word_vectors/glove.twitter.27B.' + str(emb_dim) + 'd.txt'\n",
    "    else:\n",
    "        print (\"ERROR: Please specify a correst model or SSWE cannot be loaded with embed size of: \" + str(emb_dim)) \n",
    "        return None\n",
    "    \n",
    "    embed = get_embedding_weights(vector_file, sep)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testX, testY):\n",
    "    temp = model.predict(testX)\n",
    "    y_pred  = np.argmax(temp, 1)\n",
    "    y_true = np.argmax(testY, 1)\n",
    "    precision = metrics.precision_score(y_true, y_pred, average=None)\n",
    "    recall = metrics.recall_score(y_true, y_pred, average=None)\n",
    "    f1_score = metrics.f1_score(y_true, y_pred, average=None)\n",
    "    print(\"Precision: \" + str(precision) + \"\\n\")\n",
    "    print(\"Recall: \" + str(recall) + \"\\n\")\n",
    "    print(\"f1_score: \" + str(f1_score) + \"\\n\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\":: Classification Report\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_learned_embedding(data, model_type, vector_type, embed_size, embed, vocab_processor):\n",
    "    vocab = vocab_processor.vocabulary_._mapping\n",
    "    vocab_size = len(vocab)\n",
    "    embedDict = {}\n",
    "    n = 0\n",
    "    words_missed = []\n",
    "    for k, v in vocab.iteritems():\n",
    "        try:\n",
    "            embeddingDict[v] = embed[k]\n",
    "        except:\n",
    "            n += 1\n",
    "            words_missed.append(k)\n",
    "            pass\n",
    "    print(\"%d embedding missed\"%n, \" of \" , vocab_size)\n",
    "    \n",
    "    filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + embed_size + \".pkl\"\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(embedDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(data, x_text, labels):\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split( x_text, labels, random_state=42, test_size=0.10)\n",
    "    \n",
    "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
    "    if(data != \"twitter\"):\n",
    "        max_document_length = int(np.percentile(post_length, 95))\n",
    "    else:\n",
    "        max_document_length = max(post_length)\n",
    "    print(\"Document length : \" + str(max_document_length))\n",
    "    \n",
    "    #vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
    "    #vocab_processor = vocab_processor.fit(x_text)\n",
    "\n",
    "    #trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "    #testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "    tokenizer.fit_on_texts(x_text)\n",
    "\n",
    "    trainX = tokenizer.texts_to_sequences(X_train)\n",
    "    testX = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    trainY = np.asarray(Y_train)\n",
    "    testY = np.asarray(Y_test)\n",
    "        \n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=max_document_length, value=0.)\n",
    "\n",
    "    trainY = to_categorical(trainY, nb_classes=NUM_CLASSES)\n",
    "    testY = to_categorical(testY, nb_classes=NUM_CLASSES)\n",
    "    \n",
    "    data_dict = {\n",
    "        \"data\": data,\n",
    "        \"trainX\" : trainX,\n",
    "        \"trainY\" : trainY,\n",
    "        \"testX\" : testX,\n",
    "        \"testY\" : testY,\n",
    "        \"tokenizer\": tokenizer\n",
    "        #\"vocab_processor\" : vocab_processor\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_data(data_dict):\n",
    "    #return data_dict[\"data\"], data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"vocab_processor\"]\n",
    "    return data_dict[\"data\"], data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"tokenizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_dict, model_type, vector_type, embed_size, dump_embeddings=False):\n",
    "\n",
    "    #data, trainX, trainY, testX, testY, vocab_processor = return_data(data_dict)\n",
    "    data, trainX, trainY, testX, testY, tokenizer = return_data(data_dict)\n",
    "    \n",
    "    #vocab_size = len(vocab_processor.vocabulary_)\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(vocab_size))\n",
    "    #vocab = vocab_processor.vocabulary_._mapping\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab = {i: w for w, i in word_index.items()}\n",
    "    \n",
    "    reverse_vocab = {i: w for w, i in word_index.items()}\n",
    "\n",
    "    \n",
    "    print(\"Running Model: \" + model_type + \" with word vector initiliazed with \" + vector_type + \" word vectors.\")\n",
    "    model = get_model(model_type, trainX.shape[1], vocab_size, embed_size, NUM_CLASSES, LEARN_RATE)\n",
    "\n",
    "    initial_weights = model.get_weights()\n",
    "    shuffle_weights(model, initial_weights)\n",
    "    \n",
    "    if(model_type == 'cnn'):\n",
    "        if(vector_type!=\"random\"):\n",
    "            print(\"Word vectors used: \" + vector_type)\n",
    "            embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
    "            model.set_weights(embeddingWeights, map_embedding_weights(get_embeddings_dict(vector_type, embed_size), vocab, embed_size))\n",
    "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "        else:\n",
    "            model.fit(trainX, trainY, n_epoch = EPOCHS, shuffle=True, show_metric=True, batch_size=BATCH_SIZE)\n",
    "    else:\n",
    "        if(vector_type!=\"random\"):\n",
    "            print(\"Word vectors used: \" + vector_type)\n",
    "            model.layers[0].set_weights([map_embedding_weights(get_embeddings_dict(vector_type, embed_size), vocab, embed_size)])\n",
    "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "                  verbose=1)\n",
    "        else:\n",
    "            model.fit(trainX, trainY, epochs=EPOCHS, shuffle=True, batch_size=BATCH_SIZE, \n",
    "                  verbose=1)\n",
    "            \n",
    "    if (dump_embeddings==True):\n",
    "        if(model_type == 'cnn'):\n",
    "            embeddingWeights = tflearn.get_layer_variables_by_name('EmbeddingLayer')[0]\n",
    "        else:\n",
    "            embed = model.layers[0].get_weights()[0]\n",
    "    \n",
    "        embed_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + str(embed_size) + \".pkl\"\n",
    "        embed.dump(embed_filename)\n",
    "        \n",
    "        vocab_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + str(embed_size) + \"_dict.json\"\n",
    "        reverse_vocab_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + str(embed_size) + \"_reversedict.json\"\n",
    "        \n",
    "        with open(vocab_filename, 'w') as fp:\n",
    "            #json.dump(vocab_processor.vocabulary_._mapping, fp)\n",
    "            json.dump(vocab, fp)\n",
    "        with open(reverse_vocab_filename, 'w') as fp:\n",
    "            json.dump(reverse_vocab, fp)\n",
    "    \n",
    "    return  evaluate_model(model, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(precision_scores, recall_scores, f1_scores):\n",
    "    for i in range(NUM_CLASSES):\n",
    "        print(\"\\nPrecision Class %d (avg): %0.3f (+/- %0.3f)\" % (i, precision_scores[:, i].mean(), precision_scores[:, i].std() * 2))\n",
    "        print( \"\\nRecall Class %d (avg): %0.3f (+/- %0.3f)\" % (i, recall_scores[:, i].mean(), recall_scores[:, i].std() * 2))\n",
    "        print( \"\\nF1 score Class %d (avg): %0.3f (+/- %0.3f)\" % (i, f1_scores[:, i].mean(), f1_scores[:, i].std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data, oversampling_rate):\n",
    "    \n",
    "    x_text, labels = load_data(get_filename(data)) \n",
    " \n",
    "    if(data==\"twitter\"):\n",
    "        NUM_CLASSES = 3\n",
    "        dict1 = {'racism':2,'sexism':1,'none':0}\n",
    "        labels = [dict1[b] for b in labels]\n",
    "        \n",
    "        racism = [i for i in range(len(labels)) if labels[i]==2]\n",
    "        sexism = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in racism]*(oversampling_rate-1)+ [x_text[x] for x in sexism]*(oversampling_rate-1)\n",
    "        labels = labels + [2 for i in range(len(racism))]*(oversampling_rate-1) + [1 for i in range(len(sexism))]*(oversampling_rate-1)\n",
    "    \n",
    "    else:  \n",
    "        NUM_CLASSES = 2\n",
    "        bully = [i for i in range(len(labels)) if labels[i]==1]\n",
    "        x_text = x_text + [x_text[x] for x in bully]*(oversampling_rate-1)\n",
    "        labels = list(labels) + [1 for i in range(len(bully))]*(oversampling_rate-1)\n",
    "\n",
    "    print(\"Counter after oversampling\")\n",
    "    from collections import Counter\n",
    "    print(Counter(labels))\n",
    "    \n",
    "    filter_data = []\n",
    "    for text in x_text:\n",
    "        filter_data.append(\"\".join(l for l in text if l not in string.punctuation))\n",
    "        \n",
    "    return x_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ 'cnn', 'lstm', 'blstm', 'blstm_attention']\n",
    "word_vectors = [\"random\", \"glove\" ,\"sswe\"]\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "MAX_FEATURES = 2\n",
    "NUM_CLASSES = None\n",
    "DROPOUT = 0.25\n",
    "LEARN_RATE = 0.01\n",
    "HASH_REMOVE = None\n",
    "output_folder_name = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data, oversampling_rate, model_type, vector_type, embed_size):    \n",
    "    x_text, labels = get_data(data, oversampling_rate)\n",
    "    data_dict = get_train_test(data,  x_text, labels)\n",
    "    precision, recall, f1_score = train(data_dict, model_type, vector_type, embed_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from file: data/wiki_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length : 234\n",
      "Vocabulary Size: 170996\n",
      "Running Model: blstm with word vector initiliazed with random word vectors.\n",
      "WARNING:tensorflow:From /home/suhanik/.local/lib/python3.10/site-packages/keras/initializers/initializers_v1.py:297: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/suhanik/.local/lib/python3.10/site-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/suhanik/.local/lib/python3.10/site-packages/tensorflow/python/ops/init_ops.py:93: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/suhanik/.local/lib/python3.10/site-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 20:29:00.043867: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-09 20:29:00.044184: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-09 20:29:00.044245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (suhani-desktop): /proc/driver/nvidia/version does not exist\n",
      "2023-05-09 20:29:00.067979: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-09 20:29:00.601001: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-05-09 20:29:01.082506: W tensorflow/c/c_api.cc:291] Operation '{name:'count/Assign' id:414 op device:{requested: '', assigned: ''} def:{{{node count/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count, count/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128739 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 20:29:02.417254: W tensorflow/c/c_api.cc:291] Operation '{name:'loss/mul' id:480 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/dense_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-09 20:29:02.451731: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel/v/Assign' id:1334 op device:{requested: '', assigned: ''} def:{{{node training/Adam/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel/v, training/Adam/bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128739/128739 [==============================] - 83s 642us/sample - loss: 0.5726 - acc: 0.7116\n",
      "Epoch 2/10\n",
      "128739/128739 [==============================] - 82s 635us/sample - loss: 0.5661 - acc: 0.7153\n",
      "Epoch 3/10\n",
      "128739/128739 [==============================] - 82s 634us/sample - loss: 0.5656 - acc: 0.7154\n",
      "Epoch 4/10\n",
      "128739/128739 [==============================] - 82s 636us/sample - loss: 0.5653 - acc: 0.7155\n",
      "Epoch 5/10\n",
      "128739/128739 [==============================] - 81s 633us/sample - loss: 0.5653 - acc: 0.7154\n",
      "Epoch 6/10\n",
      "128739/128739 [==============================] - 82s 634us/sample - loss: 0.5648 - acc: 0.7154\n",
      "Epoch 7/10\n",
      "128739/128739 [==============================] - 82s 634us/sample - loss: 0.5650 - acc: 0.7154\n",
      "Epoch 8/10\n",
      "128739/128739 [==============================] - 81s 629us/sample - loss: 0.5649 - acc: 0.7154\n",
      "Epoch 9/10\n",
      "128739/128739 [==============================] - 79s 613us/sample - loss: 0.5649 - acc: 0.7154\n",
      "Epoch 10/10\n",
      "128739/128739 [==============================] - 79s 612us/sample - loss: 0.5645 - acc: 0.7154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhanik/.local/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-05-09 20:42:33.764044: W tensorflow/c/c_api.cc:291] Operation '{name:'dense/Softmax' id:404 op device:{requested: '', assigned: ''} def:{{{node dense/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.71108004 0.        ]\n",
      "\n",
      "Recall: [1. 0.]\n",
      "\n",
      "f1_score: [0.83114761 0.        ]\n",
      "\n",
      "[[10172     0]\n",
      " [ 4133     0]]\n",
      ":: Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83     10172\n",
      "           1       0.00      0.00      0.00      4133\n",
      "\n",
      "    accuracy                           0.71     14305\n",
      "   macro avg       0.36      0.50      0.42     14305\n",
      "weighted avg       0.51      0.71      0.59     14305\n",
      "\n",
      "Loading data from file: data/wiki_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length : 234\n",
      "Vocabulary Size: 170996\n",
      "Running Model: blstm with word vector initiliazed with random word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 20:42:54.818450: W tensorflow/c/c_api.cc:291] Operation '{name:'bidirectional_1/forward_lstm_1/lstm_cell_4/bias/Assign' id:1517 op device:{requested: '', assigned: ''} def:{{{node bidirectional_1/forward_lstm_1/lstm_cell_4/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bidirectional_1/forward_lstm_1/lstm_cell_4/bias, bidirectional_1/forward_lstm_1/lstm_cell_4/bias/Initializer/concat)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128739 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 20:42:55.729083: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/mul' id:1915 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/dense_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-09 20:42:55.784509: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/dense_1/bias/m/Assign' id:2729 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_1/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_1/bias/m, training_2/Adam/dense_1/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128739/128739 [==============================] - 164s 1ms/sample - loss: 0.5694 - acc: 0.7132\n",
      "Epoch 2/10\n",
      "128739/128739 [==============================] - 168s 1ms/sample - loss: 0.5655 - acc: 0.7154\n",
      "Epoch 3/10\n",
      "128739/128739 [==============================] - 167s 1ms/sample - loss: 0.5649 - acc: 0.7154\n",
      "Epoch 4/10\n",
      "128739/128739 [==============================] - 167s 1ms/sample - loss: 0.5648 - acc: 0.7154\n",
      "Epoch 5/10\n",
      "128739/128739 [==============================] - 167s 1ms/sample - loss: 0.5646 - acc: 0.7154\n",
      "Epoch 6/10\n",
      "128739/128739 [==============================] - 163s 1ms/sample - loss: 0.5647 - acc: 0.7154\n",
      "Epoch 7/10\n",
      "128739/128739 [==============================] - 163s 1ms/sample - loss: 0.5643 - acc: 0.7154\n",
      "Epoch 8/10\n",
      "128739/128739 [==============================] - 163s 1ms/sample - loss: 0.5644 - acc: 0.7154\n",
      "Epoch 9/10\n",
      "128739/128739 [==============================] - 163s 1ms/sample - loss: 0.5643 - acc: 0.7154\n",
      "Epoch 10/10\n",
      "128739/128739 [==============================] - 163s 1ms/sample - loss: 0.5643 - acc: 0.7154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhanik/.local/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-05-09 21:10:22.020233: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_1/Softmax' id:1839 op device:{requested: '', assigned: ''} def:{{{node dense_1/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_1/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.71108004 0.        ]\n",
      "\n",
      "Recall: [1. 0.]\n",
      "\n",
      "f1_score: [0.83114761 0.        ]\n",
      "\n",
      "[[10172     0]\n",
      " [ 4133     0]]\n",
      ":: Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83     10172\n",
      "           1       0.00      0.00      0.00      4133\n",
      "\n",
      "    accuracy                           0.71     14305\n",
      "   macro avg       0.36      0.50      0.42     14305\n",
      "weighted avg       0.51      0.71      0.59     14305\n",
      "\n",
      "Loading data from file: data/wiki_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length : 234\n",
      "Vocabulary Size: 170996\n",
      "Running Model: blstm with word vector initiliazed with random word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 21:10:44.662515: W tensorflow/c/c_api.cc:291] Operation '{name:'bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel/Assign' id:2943 op device:{requested: '', assigned: ''} def:{{{node bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel, bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-09 21:10:44.759067: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 68398400 exceeds 10% of free system memory.\n",
      "2023-05-09 21:10:45.661877: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 68398400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128739 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 21:10:46.221261: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_2/mul' id:3350 op device:{requested: '', assigned: ''} def:{{{node loss_2/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_2/mul/x, loss_2/dense_2_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-09 21:10:46.303624: W tensorflow/c/c_api.cc:291] Operation '{name:'training_4/Adam/bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel/m/Assign' id:4130 op device:{requested: '', assigned: ''} def:{{{node training_4/Adam/bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_4/Adam/bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel/m, training_4/Adam/bidirectional_2/forward_lstm_2/lstm_cell_7/recurrent_kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-09 21:10:46.385054: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 68398400 exceeds 10% of free system memory.\n",
      "2023-05-09 21:10:46.416512: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 68398400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 21:10:46.456321: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 68398400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128739/128739 [==============================] - 382s 3ms/sample - loss: 0.5689 - acc: 0.7126\n",
      "Epoch 2/10\n",
      "128739/128739 [==============================] - 382s 3ms/sample - loss: 0.5654 - acc: 0.7154\n",
      "Epoch 3/10\n",
      "128739/128739 [==============================] - 384s 3ms/sample - loss: 0.5649 - acc: 0.7154\n",
      "Epoch 4/10\n",
      "128739/128739 [==============================] - 386s 3ms/sample - loss: 0.5646 - acc: 0.7155\n",
      "Epoch 5/10\n",
      "128739/128739 [==============================] - 384s 3ms/sample - loss: 0.5649 - acc: 0.7154\n",
      "Epoch 6/10\n",
      "128739/128739 [==============================] - 381s 3ms/sample - loss: 0.5644 - acc: 0.7154\n",
      "Epoch 7/10\n",
      "128739/128739 [==============================] - 381s 3ms/sample - loss: 0.5645 - acc: 0.7154\n",
      "Epoch 8/10\n",
      "128739/128739 [==============================] - 383s 3ms/sample - loss: 0.5644 - acc: 0.7154\n",
      "Epoch 9/10\n",
      "128739/128739 [==============================] - 385s 3ms/sample - loss: 0.5644 - acc: 0.7154\n",
      "Epoch 10/10\n",
      "128739/128739 [==============================] - 384s 3ms/sample - loss: 0.5644 - acc: 0.7153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suhanik/.local/lib/python3.10/site-packages/keras/engine/training_v1.py:2357: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2023-05-09 22:14:38.080800: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_2/Softmax' id:3274 op device:{requested: '', assigned: ''} def:{{{node dense_2/Softmax}} = Softmax[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_2/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/suhanik/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [0.71108004 0.        ]\n",
      "\n",
      "Recall: [1. 0.]\n",
      "\n",
      "f1_score: [0.83114761 0.        ]\n",
      "\n",
      "[[10172     0]\n",
      " [ 4133     0]]\n",
      ":: Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83     10172\n",
      "           1       0.00      0.00      0.00      4133\n",
      "\n",
      "    accuracy                           0.71     14305\n",
      "   macro avg       0.36      0.50      0.42     14305\n",
      "weighted avg       0.51      0.71      0.59     14305\n",
      "\n",
      "Loading data from file: data/wiki_data.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length : 234\n",
      "Vocabulary Size: 170996\n",
      "Running Model: blstm with word vector initiliazed with random word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 22:15:05.356385: W tensorflow/c/c_api.cc:291] Operation '{name:'bidirectional_3/forward_lstm_3/lstm_cell_10/recurrent_kernel/Assign' id:4378 op device:{requested: '', assigned: ''} def:{{{node bidirectional_3/forward_lstm_3/lstm_cell_10/recurrent_kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](bidirectional_3/forward_lstm_3/lstm_cell_10/recurrent_kernel, bidirectional_3/forward_lstm_3/lstm_cell_10/recurrent_kernel/Initializer/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128739 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 22:15:08.194024: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_3/mul' id:4785 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/dense_3_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-09 22:15:08.346462: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/bidirectional_3/backward_lstm_3/lstm_cell_11/bias/m/Assign' id:5589 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/bidirectional_3/backward_lstm_3/lstm_cell_11/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/bidirectional_3/backward_lstm_3/lstm_cell_11/bias/m, training_6/Adam/bidirectional_3/backward_lstm_3/lstm_cell_11/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "128739/128739 [==============================] - 986s 8ms/sample - loss: 0.5684 - acc: 0.7126\n",
      "Epoch 2/10\n",
      "128739/128739 [==============================] - 979s 8ms/sample - loss: 0.5654 - acc: 0.7154\n",
      "Epoch 3/10\n",
      "128739/128739 [==============================] - 977s 8ms/sample - loss: 0.5652 - acc: 0.7153\n",
      "Epoch 4/10\n",
      "128739/128739 [==============================] - 976s 8ms/sample - loss: 0.5648 - acc: 0.7154\n",
      "Epoch 5/10\n",
      "128739/128739 [==============================] - 975s 8ms/sample - loss: 0.5647 - acc: 0.7154\n",
      "Epoch 6/10\n",
      "128739/128739 [==============================] - 976s 8ms/sample - loss: 0.5646 - acc: 0.7154\n",
      "Epoch 7/10\n",
      "  9600/128739 [=>............................] - ETA: 15:03 - loss: 0.5577 - acc: 0.7258"
     ]
    }
   ],
   "source": [
    "data = \"wiki\"\n",
    "model_type = \"blstm\"\n",
    "vector_type = \"random\"\n",
    "\n",
    "for embed_size in [25, 50, 100, 200]:\n",
    "    run_model(data, 3, model_type, vector_type, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
