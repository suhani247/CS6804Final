{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(data, model_type, vector_type, embed_size):\n",
    "    embed_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + embed_size + \".pkl\"\n",
    "    vocab_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + embed_size + \"_dict.json\"\n",
    "    reverse_vocab_filename = output_folder_name + data + \"_\" + model_type + \"_\" + vector_type + \"_\" + embed_size + \"_reversedict.json\"\n",
    "        \n",
    "    embedding = np.load(embed_filename)\n",
    "    \n",
    "    with open(vocab_filename,\"r\") as f:\n",
    "        vocab = json.load(f)\n",
    "    with open(reverse_vocab_filename,\"r\") as f:\n",
    "        reverse_vocab = json.load(f)\n",
    "    \n",
    "    data_dict = {\n",
    "        \"data\": data,\n",
    "        \"embed\": embedding,\n",
    "        \"vocab\": vocab,\n",
    "        \"reverse_vocab\": reverse_vocab   \n",
    "    }\n",
    "        \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    plt.figure(figsize=(14, 14))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label,\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_embedding(embedding, m, n, reverse_dictionary):\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    low_dim_embs = tsne.fit_transform(embedding[m:n, :])\n",
    "    labels = [reverse_dictionary[i] for i in xrange(m,n)]\n",
    "    plot_with_labels(low_dim_embs, labels)\n",
    "    return low_dim_embs\n",
    "\n",
    "def top_similar(low_dim_embs, mapping, reverse_mapping, valid_words):\n",
    "    similarity = tf.matmul(\n",
    "          low_dim_embs, low_dim_embs, transpose_b=True)\n",
    "\n",
    "    sim = tf.Session().run(similarity)\n",
    "    for i in range(len(valid_words)):\n",
    "        if(valid_words[i] in mapping):\n",
    "            index = mapping[valid_words[i]]\n",
    "            valid_word = reverse_mapping[index]\n",
    "            top_k = 20  # number of nearest neighbors\n",
    "            nearest = (-sim[index, :][:500]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in xrange(top_k):\n",
    "                close_word = reverse_mapping[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar_clusters(data, words, embed_range=[0,500]):\n",
    "    embedding = data[\"embed\"]\n",
    "    data_dict = data[\"vocab\"]\n",
    "    data_reverse_dict = data[\"reverse_vocab\"]\n",
    "    low_dim_embs = plot_embedding(embedding, embed_range[0], embed_range[1], data_reverse_dict)\n",
    "    top_similar(low_dim_embs, data_dict, data_reverse_dict, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"formspring\"\n",
    "model_type = \"blstm_attention\"\n",
    "vector_type = \"sswe\"\n",
    "embed_size = \"50\"\n",
    "output_folder_name = \"results/\"\n",
    "\n",
    "words =  ['gay','slave','evidence','hate','fat']\n",
    "\n",
    "data_dict = get_embedding(data, model_type, vector_type, embed_size)\n",
    "\n",
    "similar_clusters(get_embedding(data, model_type, vector_type, embed_size), words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarityMatrix(low_dim_embs):\n",
    "    similarity = tf.matmul(\n",
    "          low_dim_embs, low_dim_embs, transpose_b=True)\n",
    "\n",
    "    sim = tf.Session().run(similarity)\n",
    "    return sim\n",
    "\n",
    "def similarWords(word, sim, mapping, reverse_mapping, top_k):\n",
    "    index = mapping[word]\n",
    "    log_str = ''\n",
    "    try:\n",
    "        valid_word = reverse_mapping[index]\n",
    "        nearest = (-sim[index, :][:1000]).argsort()[1:top_k + 1]\n",
    "\n",
    "        for k in xrange(top_k):\n",
    "            close_word = reverse_mapping[nearest[k]]\n",
    "            log_str = '%s%s, ' % (log_str, close_word)\n",
    "    except:\n",
    "        print \"Word beyond 10k\"\n",
    "    return log_str\n",
    "\n",
    "def get_tsneembedding(embedding, m, n):\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    low_dim_embs = tsne.fit_transform(embedding[m:n, :])\n",
    "    return low_dim_embs\n",
    "\n",
    "def comparison(dict_1, dict_2, dict_3, words, top_k):\n",
    "    for word in words:\n",
    "        print(\"Word: \" + str(word))\n",
    "        print(dict_1[\"data\"] + \": \" + similarWords(word, get_similarityMatrix(dict_1[\"embed\"]), dict_1[\"vocab\"], dict_1[\"reverse_vocab\"], top_k))\n",
    "        print(dict_2[\"data\"] + \": \" + similarWords(word, get_similarityMatrix(dict_2[\"embed\"]), dict_2[\"vocab\"], dict_2[\"reverse_vocab\"], top_k))\n",
    "        print(dict_3[\"data\"] + \": \" + similarWords(word, get_similarityMatrix(dict_3[\"embed\"]), dict_3[\"vocab\"], dict_3[\"reverse_vocab\"], top_k))       \n",
    "        print(\"----------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words =  ['gay','slave','evidence','fat','religion','ass']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
