{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/bin/python2.7')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os, sys, getopt, pickle, csv, sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, make_scorer, recall_score, precision_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble  import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "import argparse\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score    \n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [ 'svm', 'naive', 'lr', 'random_forest']\n",
    "NO_OF_FOLDS = 10\n",
    "MODEL_TYPE = \"all\"\n",
    "HASH_REMOVE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        if(HASH_REMOVE):\n",
    "            x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
    "        else:\n",
    "            x_text.append(data[i]['text'])\n",
    "        labels.append(data[i]['label'])\n",
    "    return x_text,labels\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global N_CLASS, HASH_REMOVE\n",
    "    if(dataset==\"twitter\"):\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "        N_CLASS = 3\n",
    "        HASH_REMOVE = False\n",
    "    elif(dataset==\"formspring\"):\n",
    "        N_CLASS = 2\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "        HASH_REMOVE = False\n",
    "    elif(dataset==\"wiki\"):\n",
    "        N_CLASS = 2\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "        HASH_REMOVE = False\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(y_true, y_pred):\n",
    "#     if(data==\"wiki\"):\n",
    "#         auc = roc_auc_score(y_true,y_pred)\n",
    "#         print('Test ROC AUC: %.3f' %auc)\n",
    "#     print(\":: Confusion Matrix\")\n",
    "#     print(confusion_matrix(y_true, y_pred))\n",
    "#     print(\":: Classification Report\")\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "    return np.array([ \n",
    "            precision_score(y_true, y_pred, average=None), \n",
    "            recall_score(y_true, y_pred,  average=None),\n",
    "            f1_score(y_true, y_pred, average=None)])\n",
    "    \n",
    "def print_scores(scores):\n",
    "    for i in range(N_CLASS):\n",
    "        if(i!=0):\n",
    "            print \"Precision Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:, i].mean(), scores[:, i].std() * 2)\n",
    "            print \"Recall Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:,  N_CLASS+i].mean(), scores[:,N_CLASS+i].std() * 2)\n",
    "            print \"F1_score Class %d (avg): %0.3f (+/- %0.3f)\" % (i,scores[:, N_CLASS*2+i].mean(), scores[:,  N_CLASS*2+i].std() * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_model(X, Y, model_type):\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    print \"Model Type:\", model_type\n",
    "    kf = KFold(n_splits=NO_OF_FOLDS)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        Y = np.asarray(Y)\n",
    "        model = get_model(model_type)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        curr_scores = get_scores(y_test, y_pred)\n",
    "        scores.append(np.hstack(curr_scores))\n",
    "    print_scores(np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model(m_type):\n",
    "    if m_type == 'lr':\n",
    "        logreg = LogisticRegression(class_weight=\"balanced\")\n",
    "    elif m_type == 'naive':\n",
    "        logreg =  MultinomialNB()\n",
    "    elif m_type == \"random_forest\":\n",
    "        logreg = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "    elif m_type == \"svm\":\n",
    "        logreg = LinearSVC(class_weight=\"balanced\")\n",
    "    else:\n",
    "        print \"ERROR: Please specify a correst model\"\n",
    "        return None\n",
    "    return logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x_text, labels, MODEL_TYPE):\n",
    "    \n",
    "    if(WORD):\n",
    "        print(\"Using word based features\")\n",
    "        bow_transformer = CountVectorizer(analyzer=\"word\",max_features = 10000,stop_words='english').fit(x_text)\n",
    "        comments_bow = bow_transformer.transform(x_text)\n",
    "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "        features = comments_tfidf\n",
    "    else: \n",
    "        print(\"Using char n-grams based features\")\n",
    "        bow_transformer = CountVectorizer(max_features = 10000, ngram_range = (1,2)).fit(x_text)\n",
    "        comments_bow = bow_transformer.transform(x_text)\n",
    "        tfidf_transformer = TfidfTransformer(norm = 'l2').fit(comments_bow)\n",
    "        comments_tfidf = tfidf_transformer.transform(comments_bow)\n",
    "        features = comments_tfidf\n",
    "    \n",
    "    if(data == \"twitter\"):\n",
    "        dict1 = {'racism':0,'sexism':1,'none':2}\n",
    "        labels = np.array([dict1[b] for b in labels])\n",
    "    \n",
    "    from collections import Counter\n",
    "    print(Counter(labels))\n",
    "    \n",
    "    if(MODEL_TYPE != \"all\"):\n",
    "        classification_model(features, labels, MODEL_TYPE)\n",
    "    else:\n",
    "        for model_type in models:\n",
    "            classification_model(features, labels, model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"formspring\"\n",
    "WORD =  False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"formspring\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"twitter\"\n",
    "WORD = False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"twitter\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"wiki\"\n",
    "WORD = False\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"wiki\"\n",
    "WORD = True\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "print (\"Data loaded!\")\n",
    "train(x_text, labels, MODEL_TYPE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
